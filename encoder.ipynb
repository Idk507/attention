{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "**Concept:**\n",
    "Multi-head attention is a core component of the Transformer architecture. It allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, the multi-head attention mechanism runs multiple attention functions in parallel.\n",
    "\n",
    "### Steps Involved:\n",
    "\n",
    "1. **Linear Projections:**\n",
    "   - The input embeddings are linearly projected into multiple sets of Queries (Q), Keys (K), and Values (V). This is done by multiplying the input by learned weight matrices to create different \"heads\" (subspaces).\n",
    "   - If we have \\(h\\) heads, we create \\(Q_1, Q_2, ..., Q_h\\), \\(K_1, K_2, ..., K_h\\), and \\(V_1, V_2, ..., V_h\\).\n",
    "\n",
    "2. **Scaled Dot-Product Attention:**\n",
    "   - For each head, we perform the scaled dot-product attention independently.\n",
    "   - The attention scores are computed as:\n",
    "     \\[\n",
    "     \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "     \\]\n",
    "     where \\(d_k\\) is the dimension of the key vectors (usually \\(d_k = d/h\\) where \\(d\\) is the dimension of the input embeddings).\n",
    "\n",
    "3. **Concatenation:**\n",
    "   - The outputs of all the heads are concatenated back together. This concatenated output represents the combined attention from multiple subspaces.\n",
    "   - If each head produces an output of dimension \\(d_k\\), the concatenated output will have a dimension of \\(h \\times d_k\\).\n",
    "\n",
    "4. **Final Linear Projection:**\n",
    "   - The concatenated output is linearly projected back to the original dimension \\(d\\) using another learned weight matrix.\n",
    "\n",
    "### Detailed Breakdown:\n",
    "\n",
    "1. **Linear Projections:**\n",
    "   - Given an input matrix \\(X \\in \\mathbb{R}^{n \\times d}\\) (where \\(n\\) is the sequence length and \\(d\\) is the embedding dimension), we create different sets of Queries, Keys, and Values:\n",
    "     \\[\n",
    "     Q_i = XW_i^Q, \\quad K_i = XW_i^K, \\quad V_i = XW_i^V \\quad \\text{for } i \\in \\{1, ..., h\\}\n",
    "     \\]\n",
    "     where \\(W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d \\times d_k}\\) are the learned weight matrices for the \\(i\\)-th head.\n",
    "\n",
    "2. **Scaled Dot-Product Attention for Each Head:**\n",
    "   - For each head, compute the attention scores:\n",
    "     \\[\n",
    "     \\text{Attention}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i\n",
    "     \\]\n",
    "\n",
    "3. **Concatenation:**\n",
    "   - Concatenate the outputs of all heads:\n",
    "     \\[\n",
    "     \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W^O\n",
    "     \\]\n",
    "     where \\(W^O \\in \\mathbb{R}^{hd_k \\times d}\\) is the learned weight matrix for the final linear projection.\n",
    "\n",
    "4. **Final Linear Projection:**\n",
    "   - Apply the final linear transformation to get the multi-head attention output:\n",
    "     \\[\n",
    "     \\text{Output} = \\text{Concat}(\\text{Attention}_1, \\text{Attention}_2, ..., \\text{Attention}_h)W^O\n",
    "     \\]\n",
    "\n",
    "### Advantages of Multi-Head Attention:\n",
    "\n",
    "1. **Parallel Processing:**\n",
    "   - Multiple attention heads allow the model to process different parts of the input in parallel, improving efficiency.\n",
    "\n",
    "2. **Rich Representation:**\n",
    "   - Each head learns to focus on different parts of the input, capturing various aspects and nuances of the data.\n",
    "\n",
    "3. **Better Learning Capacity:**\n",
    "   - By attending to information from different subspaces, the model can capture complex relationships and dependencies more effectively.\n",
    "\n",
    "### Example Calculation:\n",
    "\n",
    "Let's consider an example with:\n",
    "- Sequence length \\(n = 3\\)\n",
    "- Embedding dimension \\(d = 4\\)\n",
    "- Number of heads \\(h = 2\\)\n",
    "- Dimension of each head \\(d_k = 2\\)\n",
    "\n",
    "Given input \\(X\\):\n",
    "\\[\n",
    "X = \\begin{bmatrix}\n",
    "1 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 1 \\\\\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "**Linear Projections for Head 1:**\n",
    "\\[\n",
    "Q_1 = XW_1^Q, \\quad K_1 = XW_1^K, \\quad V_1 = XW_1^V\n",
    "\\]\n",
    "where \\(W_1^Q, W_1^K, W_1^V \\in \\mathbb{R}^{4 \\times 2}\\).\n",
    "\n",
    "**Attention Calculation:**\n",
    "\\[\n",
    "\\text{Attention}_1 = \\text{softmax}\\left(\\frac{Q_1K_1^T}{\\sqrt{2}}\\right)V_1\n",
    "\\]\n",
    "\n",
    "**Repeat for Head 2 and Concatenate:**\n",
    "\\[\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{Attention}_1, \\text{Attention}_2)W^O\n",
    "\\]\n",
    "\n",
    "The result is a richer representation of the input sequence, enabling the model to understand complex patterns and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math \n",
    "from torch import nn \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    print(f\"scaled.size(): {scaled.size()}\")\n",
    "    if mask is not None:\n",
    "        print(f\"Adding the Mask of shape {mask.size()}\")\n",
    "        scaled += mask \n",
    "        print(f\"scaled.size(): {scaled.size()}\")\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    print(f\"Attention.size(): {attention.size()}\")\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, max_sequence_length, d_model = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)  # (batch_size, num_heads, max_sequence_length, 3 * head_dim)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q.size: {q.size()}\")\n",
    "        print(f\"k.size: {k.size()}\")\n",
    "        print(f\"v.size: {v.size()}\")\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        print(f\"attention.size(): {attention.size()}\")\n",
    "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        variance = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (variance + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual_x = x \n",
    "        x, _ = self.attention(x, mask=None)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        print(f\"Shape of x :{x.size()}\")\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "d_model = 512 \n",
    "num_heads = 8 \n",
    "drop_prob = 0.1\n",
    "batch_size = 30 \n",
    "max_sequence_length = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(d_model,ffn_hidden,num_heads,drop_prob,num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-4): 5 x EncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNormalization()\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNormalization()\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((batch_size,max_sequence_length,d_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 200, 512])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size: torch.Size([30, 200, 1536])\n",
      "qkv.size: torch.Size([30, 200, 8, 192])\n",
      "q.size: torch.Size([30, 8, 200, 64])\n",
      "k.size: torch.Size([30, 8, 200, 64])\n",
      "v.size: torch.Size([30, 8, 200, 64])\n",
      "scaled.size(): torch.Size([30, 8, 200, 200])\n",
      "Attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64])\n",
      "attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "Shape of x :torch.Size([30, 200, 512])\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size: torch.Size([30, 200, 1536])\n",
      "qkv.size: torch.Size([30, 200, 8, 192])\n",
      "q.size: torch.Size([30, 8, 200, 64])\n",
      "k.size: torch.Size([30, 8, 200, 64])\n",
      "v.size: torch.Size([30, 8, 200, 64])\n",
      "scaled.size(): torch.Size([30, 8, 200, 200])\n",
      "Attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64])\n",
      "attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "Shape of x :torch.Size([30, 200, 512])\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size: torch.Size([30, 200, 1536])\n",
      "qkv.size: torch.Size([30, 200, 8, 192])\n",
      "q.size: torch.Size([30, 8, 200, 64])\n",
      "k.size: torch.Size([30, 8, 200, 64])\n",
      "v.size: torch.Size([30, 8, 200, 64])\n",
      "scaled.size(): torch.Size([30, 8, 200, 200])\n",
      "Attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64])\n",
      "attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "Shape of x :torch.Size([30, 200, 512])\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size: torch.Size([30, 200, 1536])\n",
      "qkv.size: torch.Size([30, 200, 8, 192])\n",
      "q.size: torch.Size([30, 8, 200, 64])\n",
      "k.size: torch.Size([30, 8, 200, 64])\n",
      "v.size: torch.Size([30, 8, 200, 64])\n",
      "scaled.size(): torch.Size([30, 8, 200, 200])\n",
      "Attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64])\n",
      "attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "Shape of x :torch.Size([30, 200, 512])\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size: torch.Size([30, 200, 1536])\n",
      "qkv.size: torch.Size([30, 200, 8, 192])\n",
      "q.size: torch.Size([30, 8, 200, 64])\n",
      "k.size: torch.Size([30, 8, 200, 64])\n",
      "v.size: torch.Size([30, 8, 200, 64])\n",
      "scaled.size(): torch.Size([30, 8, 200, 200])\n",
      "Attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64])\n",
      "attention.size(): torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "Shape of x :torch.Size([30, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "out = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.5220e-01, -1.0149e+00, -9.2678e-01,  ..., -1.0509e+00,\n",
       "          -8.8911e-01,  1.0389e+00],\n",
       "         [ 1.0142e-02, -4.3849e-01,  6.8647e-01,  ..., -2.1143e+00,\n",
       "          -8.7580e-01, -1.1077e-01],\n",
       "         [-1.5120e+00,  3.0915e-01, -1.0055e+00,  ..., -5.4164e-01,\n",
       "          -1.3031e-01, -8.3582e-01],\n",
       "         ...,\n",
       "         [ 1.3842e+00,  2.8302e-02, -7.8733e-01,  ..., -1.4032e+00,\n",
       "           4.1988e-01, -2.3569e-01],\n",
       "         [ 3.0116e-01,  7.8232e-01,  9.2847e-01,  ..., -5.4237e-01,\n",
       "           5.7213e-01,  4.7925e-01],\n",
       "         [-3.3266e-01,  4.1286e-01, -2.7234e-01,  ...,  6.6261e-02,\n",
       "           6.8822e-01, -2.4220e+00]],\n",
       "\n",
       "        [[ 1.4110e+00,  1.0553e+00, -1.4921e-01,  ..., -1.3799e+00,\n",
       "          -5.5247e-01, -1.5545e+00],\n",
       "         [ 1.5510e+00,  4.7343e-01, -1.4959e+00,  ...,  4.8307e-01,\n",
       "           3.7061e-01, -1.8659e+00],\n",
       "         [ 5.6035e-01, -7.3902e-01,  8.4590e-01,  ..., -2.7455e+00,\n",
       "          -4.0794e-01, -1.9714e+00],\n",
       "         ...,\n",
       "         [ 1.3779e+00, -3.1902e-01,  1.2544e+00,  ..., -7.3225e-01,\n",
       "           1.5116e-01, -8.7647e-01],\n",
       "         [ 9.2613e-01, -1.1321e-01, -2.3559e-01,  ...,  7.5553e-01,\n",
       "          -1.0899e+00, -1.7192e+00],\n",
       "         [ 1.0030e+00, -5.3310e-01,  3.5379e-01,  ..., -1.8668e+00,\n",
       "          -1.6529e+00, -1.1371e+00]],\n",
       "\n",
       "        [[-4.5094e-02,  9.6172e-02, -1.5542e+00,  ..., -1.8453e+00,\n",
       "          -1.6586e+00, -2.6891e+00],\n",
       "         [ 4.0800e-01,  1.3922e-01,  9.7755e-01,  ...,  1.0071e+00,\n",
       "          -6.6092e-01, -1.0615e-01],\n",
       "         [ 3.7238e-02,  5.0249e-01,  6.7213e-02,  ..., -5.1925e-01,\n",
       "           1.6825e-01, -1.0678e+00],\n",
       "         ...,\n",
       "         [-1.2155e+00, -7.0813e-01,  9.3189e-02,  ..., -1.3991e-01,\n",
       "          -7.6423e-01, -7.0225e-01],\n",
       "         [-2.6707e-01,  1.2176e+00, -7.3538e-01,  ...,  8.5475e-01,\n",
       "           1.9121e-01, -6.3148e-01],\n",
       "         [ 2.2975e+00,  4.3007e-01, -7.0261e-01,  ...,  5.0750e-01,\n",
       "           1.6715e+00, -6.4783e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.3673e-01,  5.7011e-01, -2.2956e-01,  ...,  7.1547e-01,\n",
       "           6.7711e-01, -6.5486e-01],\n",
       "         [ 6.7748e-01, -7.6987e-02,  1.8016e+00,  ..., -1.4778e+00,\n",
       "          -2.1447e+00, -1.6373e+00],\n",
       "         [-7.8386e-01, -4.4723e-01,  4.5538e-02,  ..., -1.3698e+00,\n",
       "          -1.0048e+00,  1.2100e+00],\n",
       "         ...,\n",
       "         [ 7.5876e-01, -1.1377e+00, -3.1975e-01,  ..., -1.4310e+00,\n",
       "          -5.7058e-01, -9.3737e-01],\n",
       "         [ 7.4698e-01, -2.3297e-01,  1.1404e+00,  ..., -1.5978e+00,\n",
       "          -1.5125e+00, -2.1656e+00],\n",
       "         [ 1.6504e+00,  9.3470e-02, -4.3550e-01,  ..., -3.9190e-01,\n",
       "          -9.1923e-01, -1.1189e-01]],\n",
       "\n",
       "        [[ 1.2972e+00, -4.2259e-01, -1.0107e+00,  ...,  8.7498e-01,\n",
       "           1.5102e-01, -9.4396e-01],\n",
       "         [-2.2089e-01,  4.8506e-02, -2.5124e-01,  ...,  9.6753e-01,\n",
       "          -4.2579e-01,  4.6620e-01],\n",
       "         [ 3.6977e-03,  1.0675e+00, -3.7985e-01,  ..., -3.6674e-01,\n",
       "           3.0991e+00, -1.1393e+00],\n",
       "         ...,\n",
       "         [ 1.7372e-01,  1.3366e+00, -9.3759e-01,  ..., -1.2279e+00,\n",
       "          -3.7312e-01,  1.5016e+00],\n",
       "         [ 4.6795e-02, -1.8051e+00, -7.1084e-01,  ..., -4.8117e-02,\n",
       "           6.4571e-01, -5.2329e-01],\n",
       "         [ 2.5723e+00, -2.3059e+00,  1.2248e+00,  ...,  2.7744e-01,\n",
       "          -9.6228e-01, -1.7435e+00]],\n",
       "\n",
       "        [[ 3.7532e-01,  5.5313e-01,  6.1957e-04,  ..., -6.4631e-01,\n",
       "           4.7108e-01, -1.5072e+00],\n",
       "         [ 3.1323e-01, -6.3696e-01, -1.3425e+00,  ..., -1.5287e+00,\n",
       "          -8.8507e-01, -8.5664e-01],\n",
       "         [-6.9115e-01,  9.6986e-01, -1.3172e+00,  ..., -7.3976e-01,\n",
       "          -7.3465e-01, -1.5747e+00],\n",
       "         ...,\n",
       "         [ 6.0954e-01, -6.1800e-01, -7.2757e-01,  ...,  3.0072e-01,\n",
       "          -5.0131e-01, -9.4601e-02],\n",
       "         [ 4.0839e-01, -1.0223e+00, -2.9448e-01,  ..., -1.0449e+00,\n",
       "          -2.6133e-01, -9.5157e-02],\n",
       "         [ 1.6904e+00, -1.8194e+00, -6.0141e-01,  ..., -1.4510e+00,\n",
       "           8.7910e-01, -1.0804e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
