{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,d_k,d_V = 4,8,8\n",
    "q = np.random.randn(L,d_k)\n",
    "k = np.random.randn(L,d_k)\n",
    "v = np.random.randn(L,d_V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.47079544  0.94544991  0.11432364 -0.82264383 -1.07690323 -0.8377866\n",
      "   0.89731589 -0.73518777]\n",
      " [ 0.30174324 -0.46393619  2.21484236 -2.54614492 -1.39637666  0.07252524\n",
      "  -0.24162127 -0.5056483 ]\n",
      " [-0.65268342 -0.13847951  0.58517388 -0.54466552  0.24413499 -0.99231483\n",
      "  -1.61469511  0.7233173 ]\n",
      " [ 1.29430289  0.30889733  0.64249662  2.01483537 -0.76045713  0.61302165\n",
      "  -0.76062154 -0.71551973]]\n",
      "W\n",
      " [[ 0.29847428  0.56445131  0.47308996 -0.55928965 -2.08418923  0.02623891\n",
      "  -2.03140845  1.00126964]\n",
      " [ 1.99073209  1.25118768  0.16734232  1.08565506 -1.6965257   0.90926172\n",
      "  -0.61492394  0.04818199]\n",
      " [ 1.27535146  1.07219187  0.73637664 -0.38762256  1.10281235 -1.21727865\n",
      "  -0.08541894  0.09110149]\n",
      " [-0.05756785  0.99959559 -0.19946572  0.35050611 -1.37494223 -1.55371739\n",
      "  -0.26602491  1.20038896]]\n",
      "V\n",
      " [[-0.42432474 -0.39542769 -0.36987668 -0.0304305  -1.07674195  0.86780069\n",
      "   1.38603063  1.1127637 ]\n",
      " [-0.96362591  0.06577554 -0.51002193 -2.16569437  0.71217977 -0.6653745\n",
      "  -0.82225274 -0.93073763]\n",
      " [ 0.55704868  0.32077822 -1.74594892  0.24538887 -0.13881212 -1.40600054\n",
      "  -0.21818614 -1.04844756]\n",
      " [-1.91549037 -1.94715839 -1.2201042  -0.23341425 -1.31098687  2.20645606\n",
      "   0.4856387   0.57873698]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\",q)\n",
    "print(\"W\\n\",k)\n",
    "print(\"V\\n\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.15038784,  3.7149416 ,  2.98111849,  2.21039539],\n",
       "       [ 5.19680278,  0.18576933,  0.85165031, -0.5507876 ],\n",
       "       [ 3.77797167, -2.25466583,  1.34213523,  2.09543587],\n",
       "       [ 2.16748217,  7.53881335,  0.08893905,  0.24888275]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Self Attention\n",
    "\n",
    "np.matmul(q,k.T) #softmax(Q.K^T/d_k**0.5 +M)V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0715697304249114, 1.0688839314076906, 5.314866376710878)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(),k.var(), np.matmul(q,k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0715697304249114, 1.0688839314076906, 0.6643582970888596)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "q.var(),k.var(),scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Masking  -it is required in decoding part \n",
    "\n",
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40672352,        -inf,        -inf,        -inf],\n",
       "       [ 1.83734724,  0.06567938,        -inf,        -inf],\n",
       "       [ 1.33571469, -0.79714475,  0.47451646,        -inf],\n",
       "       [ 0.76632067,  2.66537302,  0.0314447 ,  0.08799334]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax \n",
    "\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis = -1)).T\n",
    "attention = softmax(scaled+mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.85466496, 0.14533504, 0.        , 0.        ],\n",
       "       [0.64886471, 0.07688915, 0.27424614, 0.        ],\n",
       "       [0.11538562, 0.77072532, 0.0553349 , 0.05855417]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42432474, -0.39542769, -0.36987668, -0.0304305 , -1.07674195,\n",
       "         0.86780069,  1.38603063,  1.1127637 ],\n",
       "       [-0.5027041 , -0.3283987 , -0.39024469, -0.34075915, -0.81674895,\n",
       "         0.64497662,  1.06508968,  0.81577136],\n",
       "       [-0.19665329, -0.16354946, -0.75803482, -0.11896673, -0.68196965,\n",
       "         0.12633495,  0.77628734,  0.36293678],\n",
       "       [-0.87298758, -0.09119581, -0.60381935, -1.67275553,  0.34020954,\n",
       "        -0.36129294, -0.45744014, -0.6130744 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention,v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention,v)\n",
    "    return out,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values.shape: (4, 8)\n",
      "attention.shape: (4, 4)\n",
      "Values [[-0.42432474 -0.39542769 -0.36987668 -0.0304305  -1.07674195  0.86780069\n",
      "   1.38603063  1.1127637 ]\n",
      " [-0.5027041  -0.3283987  -0.39024469 -0.34075915 -0.81674895  0.64497662\n",
      "   1.06508968  0.81577136]\n",
      " [-0.19665329 -0.16354946 -0.75803482 -0.11896673 -0.68196965  0.12633495\n",
      "   0.77628734  0.36293678]\n",
      " [-0.87298758 -0.09119581 -0.60381935 -1.67275553  0.34020954 -0.36129294\n",
      "  -0.45744014 -0.6130744 ]]\n",
      "attention [[1.         0.         0.         0.        ]\n",
      " [0.85466496 0.14533504 0.         0.        ]\n",
      " [0.64886471 0.07688915 0.27424614 0.        ]\n",
      " [0.11538562 0.77072532 0.0553349  0.05855417]]\n"
     ]
    }
   ],
   "source": [
    "values,attention = scaled_dot_product_attention(q,k,v,mask=mask)\n",
    "print(\"values.shape:\",values.shape)\n",
    "print(\"attention.shape:\",attention.shape)\n",
    "print(\"Values\",values)\n",
    "print(\"attention\",attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
